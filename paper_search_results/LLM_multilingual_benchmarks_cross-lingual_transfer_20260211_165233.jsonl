{"title": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants", "year": 2023, "authors": "Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Don Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, Madian Khabsa", "url": "https://www.semanticscholar.org/paper/fe6670cfc0d0dfe184afc8e003df51333d3a750e", "relevance": 3, "abstract": "We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. We also observe that larger vocabulary size and conscious vocabulary construction correlate with better performance on low-resource languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.", "citations": 250}
{"title": "Unsupervised Cross-lingual Representation Learning at Scale", "year": 2019, "authors": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco (Paco) Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov", "url": "https://www.semanticscholar.org/paper/6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "relevance": 3, "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.", "citations": 7810}
{"title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization", "year": 2020, "authors": "Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson", "url": "https://www.semanticscholar.org/paper/ba4a34680e09e77984624c95f5245d91b54373f6", "relevance": 3, "abstract": "Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.", "citations": 1092}
{"title": "Syntax-augmented Multilingual BERT for Cross-lingual Transfer", "year": 2021, "authors": "Wasi Uddin Ahmad, Haoran Li, Kai-Wei Chang, Yashar Mehdad", "url": "https://www.semanticscholar.org/paper/dda0bce7baee7175f7b0e5f0ac81669ed1c13f07", "relevance": 3, "abstract": "In recent years, we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning. However, due to typological differences across languages, the cross-lingual transfer is challenging. Nevertheless, language syntax, e.g., syntactic dependencies, can bridge the typological gap. Previous works have shown that pre-trained multilingual encoders, such as mBERT (CITATION), capture language syntax, helping cross-lingual transfer. This work shows that explicitly providing language syntax and training mBERT using an auxiliary objective to encode the universal dependency tree structure helps cross-lingual transfer. We perform rigorous experiments on four NLP tasks, including text classification, question answering, named entity recognition, and task-oriented semantic parsing. The experiment results show that syntax-augmented mBERT improves cross-lingual transfer on popular benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across all languages. In the generalized transfer setting, the performance boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.", "citations": 38}
{"title": "LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs", "year": 2025, "authors": "Pei-Fu Guo, Yun-Da Tsai, Chun-Chia Hsu, Kai-Xin Chen, Ya-An Tsai, Kai-Wei Chang, Nanyun Peng, Mi-Yen Yeh, Shou-De Lin", "url": "https://www.semanticscholar.org/paper/5809fda34d000a4103e028b3414fa145596c4f86", "relevance": 3, "abstract": "Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.", "citations": 0}
{"title": "Trillion 7B Technical Report", "year": 2025, "authors": "Sungjun Han, Juyoung Suk, Suyeong An, Hyungguk Kim, Kyuseok Kim, Wonsuk Yang, Seungtaek Choi, Jamin Shin", "url": "https://www.semanticscholar.org/paper/36c088ba4a58db9fa891b428ca87b32c1b3c5223", "relevance": 3, "abstract": "We introduce Trillion-7B, the most token-efficient Korean-centric multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA) mechanism enables highly efficient and effective knowledge transfer from English to target languages like Korean and Japanese. Combined with optimized data mixtures, language-specific filtering, and tailored tokenizer construction, Trillion-7B achieves competitive performance while dedicating only 10\\% of its 2T training tokens to multilingual data and requiring just 59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations across 27 benchmarks in four languages demonstrate Trillion-7B's robust multilingual performance and exceptional cross-lingual consistency.", "citations": 4}
{"title": "Cross-lingual transfer of multilingual models on low resource African Languages", "year": 2024, "authors": "Harish Thangaraj, Ananya Chenat, J. Walia, V. Marivate", "url": "https://www.semanticscholar.org/paper/c4bb73bf9520731340e0ae81bf198030f2571968", "relevance": 3, "abstract": "Large multilingual models have significantly advanced natural language processing (NLP) research. However, their high resource demands and potential biases from diverse data sources have raised concerns about their effectiveness across low-resource languages. In contrast, monolingual models, trained on a single language, may better capture the nuances of the target language, potentially providing more accurate results. This study benchmarks the cross-lingual transfer capabilities from a high-resource language to a low-resource language for both, monolingual and multilingual models, focusing on Kinyarwanda and Kirundi, two Bantu languages. We evaluate the performance of transformer based architectures like Multilingual BERT (mBERT), AfriBERT, and BantuBERTa against neural-based architectures such as BiGRU, CNN, and char-CNN. The models were trained on Kinyarwanda and tested on Kirundi, with fine-tuning applied to assess the extent of performance improvement and catastrophic forgetting. AfriBERT achieved the highest cross-lingual accuracy of 88.3% after fine-tuning, while BiGRU emerged as the best-performing neural model with 83.3% accuracy. We also analyze the degree of forgetting in the original language post-fine-tuning. While monolingual models remain competitive, this study highlights that multilingual models offer strong cross-lingual transfer capabilities in resource limited settings.", "citations": 9}
{"title": "Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation", "year": 2025, "authors": "A. Snegha, Sayambhu Sen, Piyush Singh Pasi, Abhishek Singhania, Preethi Jyothi Indian Institute of Technology Bombay, Amazon Alexa", "url": "https://www.semanticscholar.org/paper/63610e24a205c2531f69f2719482cee96865f8e1", "relevance": 3, "abstract": "With the release of new large language models (LLMs) like Llama and Mistral, zero-shot cross-lingual transfer has become increasingly feasible due to their multilingual pretraining and strong generalization capabilities. However, adapting these decoder-only LLMs to new tasks across languages remains challenging. While parameter-efficient fine-tuning (PeFT) techniques like Low-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as soft prompt tuning, prefix tuning, and Llama Adapter are less explored, especially for zero-shot transfer in decoder-only models. We present a comprehensive study of three prefix-based methods for zero-shot cross-lingual transfer from English to 35+ high- and low-resource languages. Our analysis further explores transfer across linguistic families and scripts, as well as the impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix methods outperform LoRA-baselines by up to 6% on the Belebele benchmark. Similar improvements were observed with Mistral v0.3 7B as well. Despite using only 1.23M learning parameters with prefix tuning, we achieve consistent improvements across diverse benchmarks. These findings highlight the potential of prefix-based techniques as an effective and scalable alternative to LoRA, particularly in low-resource multilingual settings.", "citations": 0}
{"title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement", "year": 2024, "authors": "Lingfeng Ming, Bo Zeng, Chenyang Lyu, Tianqi Shi, Yu Zhao, Xue Yang, Yefeng Liu, Yiyu Wang, Linlong Xu, Yangyang Liu, Xiaohu Zhao, Hao Wang, Heng Liu, Hao Zhou, Huifeng Yin, Zifu Shang, Haijun Li, Longyue Wang, Weihua Luo, Kaifu Zhang", "url": "https://www.semanticscholar.org/paper/47bf93716439fb5aec0b8bf49833e8963f623104", "relevance": 3, "abstract": "Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages. To address this issue, we introduced Marco-LLM: Massive multilingual training for cross-lingual enhancement LLM. We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.", "citations": 7}
{"title": "CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer", "year": 2025, "authors": "Lavish Bansal, Naman Mishra", "url": "https://www.semanticscholar.org/paper/496fa516a92e235d3d0fda57907f585af03fe029", "relevance": 3, "abstract": "Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.", "citations": 0}
{"title": "WAD-X: Improving Zero-shot Cross-lingual Transfer via Adapter-based Word Alignment", "year": 2023, "authors": "Ahtamjan Ahmat, Yating Yang, Bo Ma, Rui Dong, Kaiwen Lu, Lei Wang", "url": "https://www.semanticscholar.org/paper/7f6a87b8d19f41917fce7430bd78731d3a50c284", "relevance": 3, "abstract": "Multilingual pre-trained language models (mPLMs) have achieved remarkable performance on zero-shot cross-lingual transfer learning. However, most mPLMs implicitly encourage cross-lingual alignment in pre-training stage, making it hard to capture accurate word alignment across languages. In this paper, we propose Word-align ADapters for Cross-lingual transfer (WAD-X) to explicitly align word representations of mPLMs via language-specific subspace. Taking a mPLM as the backbone model, WAD-X constructs subspace for each source-target language pair via adapters. The adapters use statistical alignment as the prior knowledge to guide word-level aligning in the corresponding bilingual semantic subspace. We evaluate our model across a set of target languages on three zero-shot cross-lingual transfer tasks: part-of-speech tagging (POS), dependency parsing (DP), and sentiment analysis (SA). Experimental results demonstrate that our proposed model improves zero-shot cross-lingual transfer on three benchmarks, with improvements of 2.19, 2.50, and 1.61 points in POS, DP, and SA tasks over strong baselines.", "citations": 6}
{"title": "Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM", "year": 2025, "authors": "Alexander Podolskiy, Semen Molokov, Timofey Gerasin, Maksim Titov, Alexey Rukhovich, Artem Khrapov, Kirill Morozov, Evgeny Tetin, Constantine Korikov, Pavel Efimov, Polina Lazukova, Yuliya Skripkar, Nikita Okhotnikov, Irina Piontkovskaya, Xiaojun Meng, Zou Xueyi, Zhenhe Zhang", "url": "https://www.semanticscholar.org/paper/8cae579f7bc04252161ff8edf396834a76309753", "relevance": 3, "abstract": "We present Gamayun, a 1.5B-parameter multilingual language model trained entirely from scratch on 2.5T tokens. Designed for efficiency and deployment in resource-constrained environments, Gamayun addresses the lack of research on small non-English-centric LLMs by adopting a novel two-stage pre-training strategy: balanced multilingual training for cross-lingual alignment, followed by high-quality English enrichment to transfer performance gains across languages. Our model supports 12 languages, with special focus on Russian. Despite a significantly smaller training budget than comparable models, Gamayun outperforms LLaMA3.2-1B (9T tokens) on all considered benchmarks, and surpasses Qwen2.5-1.5B (18T tokens) on a wide range of English and multilingual tasks. It matches or exceeds Qwen3 (36T tokens) on most tasks outside advanced STEM, achieving state-of-the-art results in Russian, including the MERA benchmark, among the models of comparable size (1-2B parameters).", "citations": 0}
{"title": "Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models", "year": 2024, "authors": "Sara Rajaee, C. Monz", "url": "https://www.semanticscholar.org/paper/62bfc24eb408438114f4a49466339776130b8e18", "relevance": 3, "abstract": "Recent advances in training multilingual language models on large datasets seem to have shown promising results in knowledge transfer across languages and achieve high performance on downstream tasks. However, we question to what extent the current evaluation benchmarks and setups accurately measure zero-shot cross-lingual knowledge transfer. In this work, we challenge the assumption that high zero-shot performance on target tasks reflects high cross-lingual ability by introducing more challenging setups involving instances with multiple languages. Through extensive experiments and analysis, we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages. Our findings highlight the overlooked drawbacks of existing cross-lingual test data and evaluation setups, calling for a more nuanced understanding of the cross-lingual capabilities of multilingual models.", "citations": 11}
{"title": "Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing Cross-Lingual Transfer in Low-Resource Scenarios", "year": 2025, "authors": "Gerard I. G\u00e1llego, Oriol Pareras, Mart\u00ed Cortada Garcia, Lucas Takanori, Javier Hernando", "url": "https://www.semanticscholar.org/paper/8355b58aee56bd14e8708cc85587f85339501481", "relevance": 3, "abstract": "We propose a Speech-to-Text Translation (S2TT) approach that integrates phoneme representations into a Chain-of-Thought (CoT) framework to improve translation in low-resource and zero-resource settings. By introducing phoneme recognition as an intermediate step, we enhance cross-lingual transfer, enabling translation even for languages with no labeled speech data. Our system builds on a multilingual LLM, which we extend to process speech and phonemes. Training follows a curriculum learning strategy that progressively introduces more complex tasks. Experiments on multilingual S2TT benchmarks show that phoneme-augmented CoT improves translation quality in low-resource conditions and enables zero-resource translation, while slightly impacting high-resource performance. Despite this trade-off, our findings demonstrate that phoneme-based CoT is a promising step toward making S2TT more accessible across diverse languages.", "citations": 4}
{"title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning", "year": 2020, "authors": "E. Ponti, Goran Glavavs, Olga Majewska, Qianchu Liu, Ivan Vulic, A. Korhonen", "url": "https://www.semanticscholar.org/paper/d97e7561fa7710213ccd4f8128044ea6849be377", "relevance": 3, "abstract": "In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at this http URL.", "citations": 402}
{"title": "Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer", "year": 2024, "authors": "Mingda Li, Abhijit Mishra, Utkarsh Mujumdar", "url": "https://www.semanticscholar.org/paper/2ea80e07c187ab6f3b73b4d52d40143811a618c8", "relevance": 3, "abstract": "The use of Large Language Models (LLMs) for program code generation has gained substantial attention, but their biases and limitations with non-English prompts challenge global inclusivity. This paper investigates the complexities of multilingual prompt-based code generation. Our evaluations of LLMs, including CodeLLaMa and CodeGemma, reveal significant disparities in code quality for non-English prompts; we also demonstrate the inadequacy of simple approaches like prompt translation, bootstrapped data augmentation, and finetuning. To address this, we propose a zero-shot cross-lingual approach using a neural projection technique, integrating a cross-lingual encoder like LASER to map multilingual embeddings from it into the LLM\u2019s token space. This method requires training only on English data and scales effectively to other languages. Results on a translated and quality-checked MBPP dataset show substantial improvements in code quality. This research promotes a more inclusive code generation landscape by empowering LLMs with multilingual capabilities to support the diverse linguistic spectrum in programming.", "citations": 3}
{"title": "On the Cross-lingual Transferability of Monolingual Representations", "year": 2019, "authors": "Mikel Artetxe, Sebastian Ruder, Dani Yogatama", "url": "https://www.semanticscholar.org/paper/fd6bc84144c2d77068bf3f077cb509d539f5f8e2", "relevance": 3, "abstract": "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.", "citations": 902}
{"title": "From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers", "year": 2020, "authors": "Anne Lauscher, Vinit Ravishankar, Ivan Vulic, Goran Glavas", "url": "https://www.semanticscholar.org/paper/85dc7829455819283270eb643817bcf97133464d", "relevance": 3, "abstract": "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.", "citations": 342}
{"title": "Language-specific Neurons Do Not Facilitate Cross-Lingual Transfer", "year": 2025, "authors": "S. Mondal, Sayambhu Sen, Abhishek Singhania, P. Jyothi", "url": "https://www.semanticscholar.org/paper/8ff97e924b93f1e7ce287f892d2622b8b731db83", "relevance": 3, "abstract": "Multilingual large language models (LLMs) aim towards robust natural language understanding across diverse languages, yet their performance significantly degrades on low-resource languages. This work explores whether existing techniques to identify language-specific neurons can be leveraged to enhance cross-lingual task performance of lowresource languages. We conduct detailed experiments covering existing language-specific neuron identification techniques (such as Language Activation Probability Entropy and activation probability-based thresholding) and neuron-specific LoRA fine-tuning with models like Llama 3.1 and Mistral Nemo. We find that such neuron-specific interventions are insufficient to yield cross-lingual improvements on downstream tasks (XNLI, XQuAD) in lowresource languages. This study highlights the challenges in achieving cross-lingual generalization and provides critical insights for multilingual LLMs.", "citations": 9}
{"title": "PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning", "year": 2023, "authors": "Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, W. Yu, Mengzhao Jia, Meng Jiang, Francesco Barbieri", "url": "https://www.semanticscholar.org/paper/9c60c766d65daa5554949fb3bce2df1ef58a5a16", "relevance": 3, "abstract": "Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following abilities of LLMs by 29% on average, compared to directly responding in the target language alone. Further experiments validate the versatility of our approach by employing alternative pivot languages beyond English to assist languages where LLMs exhibit lower proficiency. Our code and data are available at https://github.com/ytyz1307zzh/PLUG.", "citations": 44}
{"title": "Investigating the Potential of Task Arithmetic for Cross-Lingual Transfer", "year": 2024, "authors": "Marinela Parovic, Ivan Vulic, Anna Korhonen", "url": "https://www.semanticscholar.org/paper/0a152bd2b474e1ece1e2f0ea5fd27651cbcab77c", "relevance": 3, "abstract": "Cross-lingual transfer has recently been tackled through modular, parameter-efficient fine-tuning methods which allow arbitrary combinations of language and task modules for transfer of any task to any language. Concurrently, task arithmetic has emerged as a powerful and modular tool for editing pretrained models using multiple full fine-tunings. In this work, we connect the paradigms of task arithmetic and cross-lingual transfer, demonstrating that modularity for cross-lingual transfer can be achieved even with full model fine-tuning. Our approach displays strong performance on a range of multilingual benchmarks encompassing both high-resource and low-resource languages.", "citations": 5}
{"title": "MLQA: Evaluating Cross-lingual Extractive Question Answering", "year": 2019, "authors": "Patrick Lewis, Barlas O\u011fuz, Ruty Rinott, Sebastian Riedel, Holger Schwenk", "url": "https://www.semanticscholar.org/paper/faa5f1130ff9d05a8ae5b9ba79d675bd4d917c5d", "relevance": 3, "abstract": "Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.", "citations": 572}
{"title": "Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs", "year": 2025, "authors": "Shulin Huang, Yiran Ding, Junshu Pan, Yue Zhang", "url": "https://www.semanticscholar.org/paper/61b763922db3e3d293ecd85c54305fd1f996320f", "relevance": 3, "abstract": "Enhancing the complex reasoning capabilities of Large Language Models (LLMs) attracts widespread attention. While reinforcement learning (RL) has shown superior performance for improving complex reasoning, its impact on cross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains unexplored. We present the first systematic investigation into cross-lingual reasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation model, we conduct experiments on diverse multilingual reasoning benchmarks, including math reasoning, commonsense reasoning, and scientific reasoning. Our investigation yields two significant findings: (1) Tuning with RL not only achieves higher accuracy but also demonstrates substantially stronger cross-lingual generalization capabilities compared to SFT. (2) RL training on non-English data yields better overall performance and generalization than training on English data, which is not observed with SFT. Furthermore, through comprehensive mechanistic analyses, we explore the underlying factors of RL's superiority and generalization across languages. Our results provide compelling evidence that RL enables the model with more robust reasoning strategies, offering crucial guidance for more equitable and effective multilingual reasoning.", "citations": 2}
{"title": "Cross-lingual Transfer of Reward Models in Multilingual Alignment", "year": 2024, "authors": "Jiwoo Hong, Noah Lee, Rodrigo Mart'inez-Castano, C'esar Rodr'iguez, James Thorne", "url": "https://www.semanticscholar.org/paper/ae176cac16f67cfb133828c545460c652b71d3a3", "relevance": 3, "abstract": "Reinforcement learning with human feedback (RLHF) is shown to largely benefit from precise reward models (RMs). However, recent studies in reward modeling schemes are skewed towards English, limiting the applicability of RLHF in multilingual alignments. In this work, we investigate the cross-lingual transfer of RMs trained in diverse languages, primarily from English. Our experimental results demonstrate the strong cross-lingual transfer of English RMs, exceeding target language RMs by 3~4% average increase in Multilingual RewardBench. Furthermore, we analyze the cross-lingual transfer of RMs through the representation shifts. Finally, we perform multilingual alignment to exemplify how cross-lingual transfer in RM propagates to enhanced multilingual instruction-following capability, along with extensive analyses on off-the-shelf RMs. We release the code, model, and data.", "citations": 16}
{"title": "mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?", "year": 2024, "authors": "Tianze Hua, Tian Yun, Ellie Pavlick", "url": "https://www.semanticscholar.org/paper/ded539a2005aca58537ae33d1e20fb2fad3b5ec8", "relevance": 3, "abstract": "Many pretrained multilingual models exhibit cross-lingual transfer ability, which is often attributed to a learned language-neutral representation during pretraining. However, it remains unclear what factors contribute to the learning of a language-neutral representation, and whether the learned language-neutral representation suffices to facilitate cross-lingual transfer. We propose a synthetic task, Multilingual Othello (mOthello), as a testbed to delve into these two questions. We find that: (1) models trained with naive multilingual pretraining fail to learn a language-neutral representation across all input languages; (2) the introduction of\"anchor tokens\"(i.e., lexical items that are identical across languages) helps cross-lingual representation alignment; and (3) the learning of a language-neutral representation alone is not sufficient to facilitate cross-lingual transfer. Based on our findings, we propose a novel approach - multilingual pretraining with unified output space - that both induces the learning of language-neutral representation and facilitates cross-lingual transfer.", "citations": 16}
{"title": "English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too", "year": 2020, "authors": "Jason Phang, Phu Mon Htut, Yada Pruksachatkun, Haokun Liu, Clara Vania, Katharina Kann, Iacer Calixto, Samuel R. Bowman", "url": "https://www.semanticscholar.org/paper/b1a71677a13299755a12375f0c982484088aa9ef", "relevance": 3, "abstract": "Intermediate-task training\u2014fine-tuning a pretrained model on an intermediate task before fine-tuning again on the target task\u2014often improves model performance substantially on language understanding tasks in monolingual English settings. We investigate whether English intermediate-task training is still helpful on non-English target tasks. Using nine intermediate language-understanding tasks, we evaluate intermediate-task transfer in a zero-shot cross-lingual setting on the XTREME benchmark. We see large improvements from intermediate training on the BUCC and Tatoeba sentence retrieval tasks and moderate improvements on question-answering target tasks. MNLI, SQuAD and HellaSwag achieve the best overall results as intermediate tasks, while multi-task intermediate offers small additional improvements. Using our best intermediate-task models for each target task, we obtain a 5.4 point improvement over XLM-R Large on the XTREME benchmark, setting the state of the art as of June 2020. We also investigate continuing multilingual MLM during intermediate-task training and using machine-translated intermediate-task data, but neither consistently outperforms simply performing English intermediate-task training.", "citations": 67}
{"title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective", "year": 2025, "authors": "Wen Yang, Junhong Wu, Chong Li, Chengqing Zong, Jiajun Zhang", "url": "https://www.semanticscholar.org/paper/e2199df22da4ce45e66852b97cb387bb6759d73a", "relevance": 3, "abstract": "Recent advancements in Reinforcement Post-Training (RPT) have significantly enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased interest in the generalization of RL-based reasoning. While existing work has primarily focused on investigating its generalization across tasks or modalities, this study proposes a novel cross-linguistic perspective to investigate reasoning generalization. This raises a crucial question: $\\textit{Does the reasoning capability achieved from English RPT effectively transfer to other languages?}$ We address this by systematically evaluating English-centric LRMs on multilingual reasoning benchmarks and introducing a metric to quantify cross-lingual transferability. Our findings reveal that cross-lingual transferability varies significantly across initial model, target language, and training paradigm. Through interventional studies, we find that models with stronger initial English capabilities tend to over-rely on English-specific patterns, leading to diminished cross-lingual generalization. To address this, we conduct a thorough parallel training study. Experimental results yield three key findings: $\\textbf{First-Parallel Leap}$, a substantial leap in performance when transitioning from monolingual to just a single parallel language, and a predictable $\\textbf{Parallel Scaling Law}$, revealing that cross-lingual reasoning transfer follows a power-law with the number of training parallel languages. Moreover, we identify the discrepancy between actual monolingual performance and the power-law prediction as $\\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs fail to fully generalize across languages. Our study challenges the assumption that LRM reasoning mirrors human cognition, providing critical insights for the development of more language-agnostic LRMs.", "citations": 0}
{"title": "Cross-Prompt Encoder for Low-Performing Languages", "year": 2025, "authors": "Beso Mikaberidze, Teimuraz Saghinadze, Simon Ostermann, Philipp M\u00fcller", "url": "https://www.semanticscholar.org/paper/4d9062a9de742e7066227e4d6597500c6f952f44", "relevance": 3, "abstract": "Soft prompts have emerged as a powerful alternative to adapters in parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs) to adapt to downstream tasks without architectural changes or parameter updates. While prior work has focused on stabilizing training via parameter interaction in small neural prompt encoders, their broader potential for transfer across languages remains unexplored. In this paper, we demonstrate that a prompt encoder can play a central role in improving performance on low-performing languages - those that achieve poor accuracy even under full-model fine-tuning. We investigate a lightweight encoder paired with multi-source training on typologically diverse languages. We call this architecture-training combination the Cross-Prompt Encoder (XPE), and show that it advances the capture of abstract, transferable patterns across languages. To complement XPE, we propose a Dual Soft Prompt mechanism that combines an encoder-based prompt with a directly trained standard soft prompt. This hybrid design proves especially effective for target languages that benefit from both broadly shared structure and language-specific alignment. Text classification experiments with a transformer encoder (XLM-R) on the SIB-200 benchmark reveal a consistent trade-off: XPE is most effective for low-performing languages, while hybrid variants offer broader adaptability across multilingual settings.", "citations": 0}
{"title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation", "year": 2021, "authors": "Sebastian Ruder, Noah Constant, Jan A. Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Graham Neubig, Melvin Johnson", "url": "https://www.semanticscholar.org/paper/2b9762e91305986ac8a2d624d0a69521304405f3", "relevance": 3, "abstract": "Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.", "citations": 215}
{"title": "Composable Sparse Fine-Tuning for Cross-Lingual Transfer", "year": 2021, "authors": "Alan Ansell, E. Ponti, A. Korhonen, Ivan Vulic", "url": "https://www.semanticscholar.org/paper/fc58779940abb92166b73f47867763a07368c739", "relevance": 3, "abstract": "Fine-tuning the entire set of parameters of a large pretrained model has become the mainstream approach for transfer learning. To increase its efficiency and prevent catastrophic forgetting and interference, techniques like adapters and sparse fine-tuning have been developed. Adapters are modular, as they can be combined to adapt a model towards different facets of knowledge (e.g., dedicated language and/or task adapters). Sparse fine-tuning is expressive, as it controls the behavior of all model components. In this work, we introduce a new fine-tuning method with both these desirable properties. In particular, we learn sparse, real-valued masks based on a simple variant of the Lottery Ticket Hypothesis. Task-specific masks are obtained from annotated data in a source language, and language-specific masks from masked language modeling in a target language. Both these masks can then be composed with the pretrained model. Unlike adapter-based fine-tuning, this method neither increases the number of parameters at inference time nor alters the original model architecture. Most importantly, it outperforms adapters in zero-shot cross-lingual transfer by a large margin in a series of multilingual benchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we additionally find that sparsity is crucial to prevent both 1) interference between the fine-tunings to be composed and 2) overfitting. We release the code and models at https://github.com/cambridgeltl/composable-sft.", "citations": 169}
{"title": "NeoBabel: A Multilingual Open Tower for Visual Generation", "year": 2025, "authors": "Mohammad Mahdi Derakhshani, Dheeraj Varghese, Marzieh Fadaee, Cees G. M. Snoek", "url": "https://www.semanticscholar.org/paper/eb4fb6b018fd5ccf2d590faf0eb090aab90c999e", "relevance": 3, "abstract": "Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, a novel multilingual image generation framework that sets a new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using a combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 2-4x smaller. We release an open toolkit, including all code, model checkpoints, a curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not a trade-off but a catalyst for improved robustness, efficiency, and cultural fidelity in generative AI.", "citations": 1}
{"title": "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT", "year": 2019, "authors": "Shijie Wu, Mark Dredze", "url": "https://www.semanticscholar.org/paper/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3", "relevance": 3, "abstract": "Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.", "citations": 724}
{"title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer", "year": 2020, "authors": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel", "url": "https://www.semanticscholar.org/paper/74276a37bfa50f90dfae37f767b2b67784bd402a", "relevance": 3, "abstract": "The recent \u201cText-to-Text Transfer Transformer\u201d (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent \u201caccidental translation\u201d in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.", "citations": 2993}
{"title": "Cross-lingual Language Model Pretraining", "year": 2019, "authors": "Guillaume Lample, Alexis Conneau", "url": "https://www.semanticscholar.org/paper/ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "relevance": 3, "abstract": "Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT\u201916 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT\u201916 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.", "citations": 2917}
{"title": "Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer", "year": 2023, "authors": "Fei Wang, Kuan-Hao Huang, Kai-Wei Chang, Muhao Chen", "url": "https://www.semanticscholar.org/paper/454f8639761c3eedf762cfad94c0f79d7e6143d2", "relevance": 3, "abstract": "Zero-shot cross-lingual transfer is a central task in multilingual NLP, allowing models trained in languages with more sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain. In this paper, we propose a simple yet effective method, SALT, to improve the zero-shot cross-lingual transfer of the multilingual pretrained language models without the help of such external data. By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM and enhances its transferability on downstream tasks. Experimental results on XNLI and PAWS-X show that our method is able to improve zero-shot cross-lingual transferability without external data. Our code is available at https://github.com/luka-group/SALT.", "citations": 6}
{"title": "Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP", "year": 2025, "authors": "Poli Nemkova, Amrit Adhikari, Matthew Pearson, Vamsi Krishna Sadu, Mark V. Albert", "url": "https://www.semanticscholar.org/paper/e304cca65cb72b7bb2f0c3b04bc756c5e3732dc1", "relevance": 3, "abstract": "Humanitarian organizations face a critical choice: invest in costly commercial APIs or rely on free open-weight models for multilingual human rights monitoring. While commercial systems offer reliability, open-weight alternatives lack empirical validation -- especially for low-resource languages common in conflict zones. This paper presents the first systematic comparison of commercial and open-weight large language models (LLMs) for human-rights-violation detection across seven languages, quantifying the cost-reliability trade-off facing resource-constrained organizations. Across 78,000 multilingual inferences, we evaluate six models -- four instruction-aligned (Claude-Sonnet-4, DeepSeek-V3, Gemini-Flash-2.0, GPT-4.1-mini) and two open-weight (LLaMA-3-8B, Mistral-7B) -- using both standard classification metrics and new measures of cross-lingual reliability: Calibration Deviation (CD), Decision Bias (B), Language Robustness Score (LRS), and Language Stability Score (LSS). Results show that alignment, not scale, determines stability: aligned models maintain near-invariant accuracy and balanced calibration across typologically distant and low-resource languages (e.g., Lingala, Burmese), while open-weight models exhibit significant prompt-language sensitivity and calibration drift. These findings demonstrate that multilingual alignment enables language-agnostic reasoning and provide practical guidance for humanitarian organizations balancing budget constraints with reliability in multilingual deployment.", "citations": 1}
{"title": "PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification", "year": 2019, "authors": "Yinfei Yang, Y. Zhang, C. Tar, Jason Baldridge", "url": "https://www.semanticscholar.org/paper/04a7021fe6be6bddcfae476493fcc7571e7c613c", "relevance": 3, "abstract": "Most existing work on adversarial data generation focuses on English. For example, PAWS (Paraphrase Adversaries from Word Scrambling) consists of challenging English paraphrase identification pairs from Wikipedia and Quora. We remedy this gap with PAWS-X, a new dataset of 23,659 human translated PAWS evaluation pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual BERT fine-tuned on PAWS English plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-English languages and an average accuracy gain of 23% over the next best model. PAWS-X shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information.", "citations": 406}
{"title": "The Impact of Language Adapters in Cross-Lingual Transfer for NLU", "year": 2024, "authors": "Jenny Kunz, Oskar Holmstr\u00f6m", "url": "https://www.semanticscholar.org/paper/418b8f3b2b2b0126cd2f4732ad0a3172c60bc6ea", "relevance": 3, "abstract": "Modular deep learning has been proposed for the efficient adaption of pre-trained models to new tasks, domains and languages. In particular, combining language adapters with task adapters has shown potential where no supervised data exists for a language. In this paper, we explore the role of language adapters in zero-shot cross-lingual transfer for natural language understanding (NLU) benchmarks. We study the effect of including a target-language adapter in detailed ablation studies with two multilingual models and three multilingual datasets. Our results show that the effect of target-language adapters is highly inconsistent across tasks, languages and models. Retaining the source-language adapter instead often leads to an equivalent, and sometimes to a better, performance. Removing the language adapter after training has only a weak negative effect, indicating that the language adapters do not have a strong impact on the predictions.", "citations": 7}
{"title": "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond", "year": 2018, "authors": "Mikel Artetxe, Holger Schwenk", "url": "https://www.semanticscholar.org/paper/160563abbd75265b19afc8b4169bab9e1eb33d97", "relevance": 3, "abstract": "Abstract We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared byte-pair encoding vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our experiments in cross-lingual natural language inference (XNLI data set), cross-lingual document classification (MLDoc data set), and parallel corpus mining (BUCC data set) show the effectiveness of our approach. We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low- resource languages. Our implementation, the pre-trained encoder, and the multilingual test set are available at https://github.com/facebookresearch/LASER.", "citations": 1105}
{"title": "Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator", "year": 2024, "authors": "Xiaoyu Qiu, Yuechen Wang, Jiaxin Shi, Wen-gang Zhou, Houqiang Li", "url": "https://www.semanticscholar.org/paper/1ea077aa7cfe8f7d1a13305b37d3cb831b5889dc", "relevance": 3, "abstract": "Based on multilingual pre-trained models, cross-lingual transfer with prompt learning has shown promising effectiveness, where soft prompt learned in a source language is transferred to target languages for downstream tasks, particularly in the low-resource scenario. To efficiently transfer soft prompt, we propose a novel framework, Multilingual Prompt Translator (MPT), where a multilingual prompt translator is introduced to properly process crucial knowledge embedded in prompt by changing language knowledge while retaining task knowledge. More concretely, we first train prompt in source language and employ translator to translate it into target prompt. Besides, we extend an external corpus as auxiliary data, on which an alignment task for predicted answer probability is designed to convert language knowledge, thereby equipping target prompt with multilingual knowledge. In few-shot settings on XNLI, MPT demonstrates superiority over baselines by remarkable improvements. MPT is more prominent compared with vanilla prompting when transferring to languages quite distinct from source language. Code is available at https://github.com/qiuxiaoyu9954/MPT.", "citations": 7}
{"title": "Contextual Label Projection for Cross-Lingual Structured Prediction", "year": 2023, "authors": "Tanmay Parekh, I-Hung Hsu, Kuan-Hao Huang, Kai-Wei Chang, Nanyun Peng", "url": "https://www.semanticscholar.org/paper/8810d907269b70db2f4bbbc5d49bb376a7423928", "relevance": 3, "abstract": "Label projection, which involves obtaining translated labels and texts jointly, is essential for leveraging machine translation to facilitate cross-lingual transfer in structured prediction tasks. Prior research exploring label projection often compromise translation accuracy by favoring simplified label translation or relying solely on word-level alignments. In this paper, we introduce a novel label projection approach, CLaP, which translates text to the target language and performs *contextual translation* on the labels using the translated text as the context, ensuring better accuracy for the translated labels. We leverage instruction-tuned language models with multilingual capabilities as our contextual translator, imposing the constraint of the presence of translated labels in the translated text via instructions. We benchmark CLaP with other label projection techniques on zero-shot cross-lingual transfer across 39 languages on two representative structured prediction tasks - event argument extraction (EAE) and named entity recognition (NER), showing over 2.4 F1 improvement for EAE and 1.4 F1 improvement for NER. We further explore the applicability of CLaP on ten extremely low-resource languages to showcase its potential for cross-lingual structured prediction.", "citations": 11}
{"title": "Event Extraction in Basque: Typologically Motivated Cross-Lingual Transfer-Learning Analysis", "year": 2024, "authors": "Mikel Zubillaga, Oscar Sainz, A. Estarrona, Oier L\u00f3pez de Lacalle, Eneko Agirre", "url": "https://www.semanticscholar.org/paper/07f78148576ad878bbff90af69806cd573f6486c", "relevance": 3, "abstract": "Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language. This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic. We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages. Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on transfer quality. Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual transfer. In contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature. In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting. To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE). The dataset and code are publicly available.", "citations": 6}
{"title": "A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics", "year": 2025, "authors": "Lu\u00edsa Shimabucoro, A. Ustun, Marzieh Fadaee, Sebastian Ruder", "url": "https://www.semanticscholar.org/paper/4c8fd6c422f133c1cff5016137f8e2f5ad82a838", "relevance": 2, "abstract": "In order for large language models to be useful across the globe, they are fine-tuned to follow instructions on multilingual data. Despite the ubiquity of such post-training, a clear understanding of the dynamics that enable cross-lingual transfer remains elusive. This study examines cross-lingual transfer (CLT) dynamics in realistic post-training settings. We study two model families of up to 35B parameters in size trained on carefully controlled mixtures of multilingual data on three generative tasks with varying levels of complexity (summarization, instruction following, and mathematical reasoning) in both single-task and multi-task instruction tuning settings. Overall, we find that the dynamics of cross-lingual transfer and multilingual performance cannot be explained by isolated variables, varying depending on the combination of post-training settings. Finally, we identify the conditions that lead to effective cross-lingual transfer in practice.", "citations": 4}
{"title": "Probing the Emergence of Cross-lingual Alignment during LLM Training", "year": 2024, "authors": "Hetong Wang, Pasquale Minervini, E. Ponti", "url": "https://www.semanticscholar.org/paper/3b869bd70517c9b3b74bed854c30ae42060c2b20", "relevance": 2, "abstract": "Multilingual Large Language Models (LLMs) achieve remarkable levels of zero-shot cross-lingual transfer performance. We speculate that this is predicated on their ability to align languages without explicit supervision from parallel sentences. While representations of translationally equivalent sentences in different languages are known to be similar after convergence, however, it remains unclear how such cross-lingual alignment emerges during pre-training of LLMs. Our study leverages intrinsic probing techniques, which identify which subsets of neurons encode linguistic features, to correlate the degree of cross-lingual neuron overlap with the zero-shot cross-lingual transfer performance for a given model. In particular, we rely on checkpoints of BLOOM, a multilingual autoregressive LLM, across different training steps and model scales. We observe a high correlation between neuron overlap and downstream performance, which supports our hypothesis on the conditions leading to effective cross-lingual transfer. Interestingly, we also detect a degradation of both implicit alignment and multilingual abilities in certain phases of the pre-training process, providing new insights into the multilingual pretraining dynamics.", "citations": 33}
{"title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs", "year": 2025, "authors": "Danni Liu, Jan Niehues", "url": "https://www.semanticscholar.org/paper/ff6cee9ee319579546fadd890e69974c8ec4672a", "relevance": 2, "abstract": "While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. Our code is publicly available (https://github.com/dannigt/mid-align).", "citations": 13}
{"title": "Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training", "year": 2025, "authors": "Linjuan Wu, Haoran Wei, Huan Lin, Tianhao Li, Baosong Yang, Weiming Lu", "url": "https://www.semanticscholar.org/paper/684c0d9639e4fdddd50a903bac58096b3a6c992d", "relevance": 2, "abstract": "Large language models (LLMs) exhibit remarkable multilingual capabilities despite English-dominated pre-training, attributed to cross-lingual mechanisms during pre-training. Existing methods for enhancing cross-lingual transfer remain constrained by parallel resources, suffering from limited linguistic and domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT), a simple and scalable approach that enhances cross-lingual transfer by leveraging semantically related bilingual texts via simple next-word prediction. We construct CrossIC-PT samples by interleaving semantic-related bilingual Wikipedia documents into a single context window. To access window size constraints, we implement a systematic segmentation policy to split long bilingual document pairs into chunks while adjusting the sliding window mechanism to preserve contextual coherence. We further extend data availability through a semantic retrieval framework to construct CrossIC-PT samples from web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%, 3.99%, and 1.95%, respectively, with additional improvements after data augmentation.", "citations": 4}
{"title": "Zero-shot cross-lingual transfer in instruction tuning of large language models", "year": 2024, "authors": "Nadezhda Chirkova, Vassilina Nikoulina", "url": "https://www.semanticscholar.org/paper/ad9330c7cb31adb2197dae8f7f9998142e071857", "relevance": 2, "abstract": "Instruction tuning (IT) is widely used to teach pretrained large language models (LLMs) to follow arbitrary instructions, but is under-studied in multilingual settings. In this work, we conduct a systematic study of zero-shot cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only data and then tested on user prompts in other languages. We advocate for the importance of evaluating various aspects of model responses in multilingual instruction following and investigate the influence of different model configuration choices. We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data. English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in other languages, but suffer from low factuality and may occasionally have fluency errors.", "citations": 9}
{"title": "Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets", "year": 2024, "authors": "Shadi Manafi, Nikhil Krishnaswamy", "url": "https://www.semanticscholar.org/paper/20e6b4ad254ad577fd703a16730f4065ab582df0", "relevance": 2, "abstract": "Multilingual Language Models (MLLMs) exhibit robust cross-lingual transfer capabilities, or the ability to leverage information acquired in a source language and apply it to a target language. These capabilities find practical applications in well-established Natural Language Processing (NLP) tasks such as Named Entity Recognition (NER). This study aims to investigate the effectiveness of a source language when applied to a target language, particularly in the context of perturbing the input test set. We evaluate on 13 pairs of languages, each including one high-resource language (HRL) and one low-resource language (LRL) with a geographic, genetic, or borrowing relationship. We evaluate two well-known MLLMs\u2014MBERT and XLM-R\u2014on these pairs, in native LRL and cross-lingual transfer settings, in two tasks, under a set of different perturbations. Our findings indicate that NER cross-lingual transfer depends largely on the overlap of entity chunks. If a source and target language have more entities in common, the transfer ability is stronger. Models using cross-lingual transfer also appear to be somewhat more robust to certain perturbations of the input, perhaps indicating an ability to leverage stronger representations derived from the HRL. Our research provides valuable insights into cross-lingual transfer and its implications for NLP applications, and underscores the need to consider linguistic nuances and potential limitations when employing MLLMs across distinct languages.", "citations": 3}
{"title": "An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models", "year": 2024, "authors": "FAHIM FAISAL, Antonios Anastasopoulos", "url": "https://www.semanticscholar.org/paper/41173fa8cb93f51a4f8e2c023a2767c4f03e2ad0", "relevance": 2, "abstract": "The capacity and effectiveness of pre-trained multilingual models (MLMs) for zero-shot cross-lingual transfer is well established. However, phenomena of positive or negative transfer, and the effect of language choice still need to be fully understood, especially in the complex setting of massively multilingual LMs. We propose an efficient method to study transfer language influence in zero-shot performance on another target language. Unlike previous work, our approach disentangles downstream tasks from language, using dedicated adapter units. Our findings suggest that some languages do not largely affect others, while some languages, especially ones unseen during pre-training, can be extremely beneficial or detrimental for different target languages. We find that no transfer language is beneficial for all target languages. We do, curiously, observe languages previously unseen by MLMs consistently benefit from transfer from almost any language. We additionally use our modular approach to quantify negative interference efficiently and categorize languages accordingly. Furthermore, we provide a list of promising transfer-target language configurations that consistently lead to target language performance improvements.", "citations": 4}
{"title": "The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs", "year": 2025, "authors": "Lucas Bandarkar, Nanyun Peng", "url": "https://www.semanticscholar.org/paper/5ec405b200c5970418aae2df5228354fcc8eef10", "relevance": 2, "abstract": "Large language models (LLMs) still struggle across tasks outside of high-resource languages. In this work, we investigate cross-lingual transfer to lower-resource languages where task-specific post-training data is scarce. Building on prior work, we first validate that the subsets of model parameters that matter most for mathematical reasoning and multilingual capabilities are distinctly non-overlapping. To exploit this implicit separability between task and target language parameterization, we develop and analyze numerous modular frameworks to improve the composition of the two during fine-tuning. These methods generally employ freezing parameters or post hoc model merging to assign math and language improvement to different key parts of the LLM. In the absence of in-language math data, we demonstrate that the modular approaches successfully improve upon baselines across three languages, four models, and two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most consistently successful modular method to be fine-tuning separate language and math experts and model merging via Layer-Swapping, somewhat surprisingly. We offer possible explanations for this result via recent works on the linearity of task vectors. We further explain this by empirically showing that reverting less useful fine-tuning updates after training often outperforms freezing them from the start.", "citations": 1}
{"title": "MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer", "year": 2020, "authors": "Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, Sebastian Ruder", "url": "https://www.semanticscholar.org/paper/26299d5fdc5137291dc6a091573b3d18aba1d1c2", "relevance": 2, "abstract": "The main goal behind state-of-the-art pretrained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pretraining. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pretrained multilingual model to a new language. MAD-X outperforms the state of the art in cross-lingual transfer across a representative set of typologically diverse languages on named entity recognition and achieves competitive results on question answering.", "citations": 711}
{"title": "Self-Translate-Train: Enhancing Cross-Lingual Transfer of Large Language Models via Inherent Capability", "year": 2024, "authors": "Ryokan Ri, Shun Kiyono, Sho Takase", "url": "https://www.semanticscholar.org/paper/7a033c23f0f923697b44a24e90376a2d5ba50f79", "relevance": 2, "abstract": "Zero-shot cross-lingual transfer by fine-tuning multilingual pretrained models shows promise for low-resource languages, but often suffers from misalignment of internal representations between languages. We hypothesize that even when the model cannot generalize across languages effectively in fine-tuning, it still captures cross-lingual correspondence useful for cross-lingual transfer. We explore this hypothesis with Self-Translate-Train, a method that lets large language models (LLMs) to translate training data into the target language and fine-tunes the model on its own generated data. By demonstrating that Self-Translate-Train outperforms zero-shot transfer, we encourage further exploration of better methods to elicit cross-lingual capabilities of LLMs.", "citations": 2}
{"title": "Comparing LLM prompting with Cross-lingual transfer performance on Indigenous and Low-resource Brazilian Languages", "year": 2024, "authors": "David Ifeoluwa Adelani, A. S. Dougruoz, Andr'e Coneglian, Atul Kr. Ojha", "url": "https://www.semanticscholar.org/paper/71cf75ea133313897a272841809c3ef6c17c72bf", "relevance": 2, "abstract": "Large Language Models are transforming NLP for a lot of tasks. However, how LLMs perform NLP tasks for LRLs is less explored. In alliance with the theme track of the NAACL\u201924, we focus on 12 low-resource languages (LRLs) from Brazil, 2 LRLs from Africa and 2 high-resource languages (HRLs) (e.g., English and Brazilian Portuguese). Our results indicate that the LLMs perform worse for the labeling of LRLs in comparison to HRLs in general. We explain the reasons behind this failure and provide an error analyses through examples from 2 Brazilian LRLs.", "citations": 8}
{"title": "Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer for Decoder Language Models", "year": 2024, "authors": "Divyanshu Aggarwal, Ashutosh Sathe, Sunayana Sitaram", "url": "https://www.semanticscholar.org/paper/19c062cc5d9530217c754026e5e668d03e04cfb4", "relevance": 2, "abstract": "Large Language Models (LLMs) demonstrate exceptional capabilities in a multitude of NLP tasks. However, the efficacy of such models to languages other than English is often limited. Prior works have shown that encoder-only models such as BERT or XLM-RoBERTa show impressive cross lingual transfer of their capabilities from English to other languages. In this work, we propose a pretraining strategy that uses active forgetting to achieve similar cross lingual transfer in decoder-only LLMs. We show that LLMs pretrained with active forgetting are highly effective when adapting to new and unseen languages. Through extensive experimentation, we find that LLMs pretrained with active forgetting are able to learn better multilingual representations which translates to better performance in many downstream tasks.", "citations": 4}
{"title": "How does a Multilingual LM Handle Multiple Languages?", "year": 2025, "authors": "Santhosh Kakarla, Gautama Shastry Bulusu Venkata, Aishwarya Gaddam", "url": "https://www.semanticscholar.org/paper/e17eb97a08184783769920a1e0909eb2a74490f0", "relevance": 2, "abstract": "Multilingual language models have significantly advanced due to rapid progress in natural language processing. Models like BLOOM 1.7B, trained on diverse multilingual datasets, aim to bridge linguistic gaps. However, their effectiveness in capturing linguistic knowledge, particularly for low-resource languages, remains an open question. This study critically examines MLMs capabilities in multilingual understanding, semantic representation, and cross-lingual knowledge transfer. While these models perform well for high-resource languages, they struggle with less-represented ones. Additionally, traditional evaluation methods often overlook their internal syntactic and semantic encoding. This research addresses key limitations through three objectives. First, it assesses semantic similarity by analyzing multilingual word embeddings for consistency using cosine similarity. Second, it examines BLOOM-1.7B and Qwen2 through Named Entity Recognition and sentence similarity tasks to understand their linguistic structures. Third, it explores cross-lingual knowledge transfer by evaluating generalization from high-resource to low-resource languages in sentiment analysis and text classification. By leveraging linguistic probing, performance metrics, and visualizations, this study provides insights into the strengths and limitations of MLMs. The findings aim to enhance multilingual NLP models, ensuring better support for both high- and low-resource languages, thereby promoting inclusivity in language technologies.", "citations": 1}
{"title": "ContrastiveMix: Overcoming Code-Mixing Dilemma in Cross-Lingual Transfer for Information Retrieval", "year": 2024, "authors": "Junggeun Do, Jaeseong Lee, Seung-won Hwang", "url": "https://www.semanticscholar.org/paper/f7999236af17c0ca41a080f58b121ad8b9a78b7b", "relevance": 2, "abstract": "Multilingual pretrained language models (mPLMs) have been widely adopted in cross-lingual transfer, and code-mixing has demonstrated effectiveness across various tasks in the absence of target language data. Our contribution involves an in-depth investigation into the counterproductive nature of training mPLMs on code-mixed data for information retrieval (IR). Our finding is that while code-mixing demonstrates a positive effect in aligning representations across languages, it hampers the IR-specific objective of matching representations between queries and relevant passages. To balance between positive and negative effects, we introduce ContrastiveMix, which disentangles contrastive loss between these conflicting objectives, thereby enhancing zero-shot IR performance. Specifically, we leverage both English and code-mixed data and employ two contrastive loss functions, by adding an additional contrastive loss that aligns embeddings of English data with their code-mixed counterparts in the query encoder. Our proposed ContrastiveMix exhibits statistically significant outperformance compared to mDPR, particularly in scenarios involving lower linguistic similarity, where the conflict between goals is more pronounced.", "citations": 4}
{"title": "Free Lunch: Robust Cross-Lingual Transfer via Model Checkpoint Averaging", "year": 2023, "authors": "Fabian David Schmidt, Ivan Vulic, Goran Glavavs", "url": "https://www.semanticscholar.org/paper/ef823957099c8a5e7e46fbb9b630a9c4e0ef6692", "relevance": 2, "abstract": "Massively multilingual language models have displayed strong performance in zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer setups, where models fine-tuned on task data in a source language are transferred without any or with only a few annotated instances to the target language(s). However, current work typically overestimates model performance as fine-tuned models are frequently evaluated at model checkpoints that generalize best to validation instances in the target languages. This effectively violates the main assumptions of \u2018true\u2019 ZS-XLT and FS-XLT. Such XLT setups require robust methods that do not depend on labeled target language data for validation and model selection. In this work, aiming to improve the robustness of \u2018true\u2019 ZS-XLT and FS-XLT, we propose a simple and effective method that averages different checkpoints (i.e., model snapshots) during task fine-tuning. We conduct exhaustive ZS-XLT and FS-XLT experiments across higher-level semantic tasks (NLI, extractive QA) and lower-level token classification tasks (NER, POS). The results indicate that averaging model checkpoints yields systematic and consistent performance gains across diverse target languages in all tasks. Importantly, it simultaneously substantially desensitizes XLT to varying hyperparameter choices in the absence of target language validation. We also show that checkpoint averaging benefits performance when further combined with run averaging (i.e., averaging the parameters of models fine-tuned over independent runs).", "citations": 11}
{"title": "Typologically Informed Parameter Aggregation", "year": 2026, "authors": "Stef Accou, Wessel Poelman", "url": "https://www.semanticscholar.org/paper/dc3e81a4ca7a9b7966cde7a36767b5fdaac91592", "relevance": 2, "abstract": "Massively multilingual language models enable cross-lingual generalization but underperform on low-resource and unseen languages. While adapter-based fine-tuning offers a parameter-efficient solution, training language-specific adapters at scale remains costly. We introduce Typologically Informed Parameter Aggregation (TIPA), a training-free method that constructs proxy language adapters by aggregating existing ones, weighted by typological similarity. Integrated into the MAD-X framework, these proxies enable zero-shot cross-lingual transfer without additional training. We evaluate TIPA on five NLP tasks and over 230 languages. TIPA consistently outperforms or matches baselines such as English-only fine-tuning or selecting the typologically closest language adapter. We see the largest gains for languages lacking dedicated adapters. Our results demonstrate that typologically informed aggregation provides a viable alternative to language-specific modules without any training needed.", "citations": 0}
{"title": "Analysis of Multi-Source Language Training in Cross-Lingual Transfer", "year": 2024, "authors": "Seong Hoon Lim, Taejun Yun, Jinhyeon Kim, Jihun Choi, Taeuk Kim", "url": "https://www.semanticscholar.org/paper/116a5a23bf9a1486e41a1c3a140cc25f33a3ecea", "relevance": 2, "abstract": "The successful adaptation of multilingual language models (LMs) to a specific language-task pair critically depends on the availability of data tailored for that condition. While cross-lingual transfer (XLT) methods have contributed to addressing this data scarcity problem, there still exists ongoing debate about the mechanisms behind their effectiveness. In this work, we focus on one of promising assumptions about inner workings of XLT, that it encourages multilingual LMs to place greater emphasis on language-agnostic or task-specific features. We test this hypothesis by examining how the patterns of XLT change with a varying number of source languages involved in the process. Our experimental findings show that the use of multiple source languages in XLT-a technique we term Multi-Source Language Training (MSLT)-leads to increased mingling of embedding spaces for different languages, supporting the claim that XLT benefits from making use of language-independent information. On the other hand, we discover that using an arbitrary combination of source languages does not always guarantee better performance. We suggest simple heuristics for identifying effective language combinations for MSLT and empirically prove its effectiveness.", "citations": 5}
{"title": "Is Cross-lingual Evaluation Only About Cross-lingual?", "year": 2021, "authors": "Wasi Ahmad, Haoran Li, Kai-Wei Chang, Mikel Artetxe, Holger Schwenk, Mas-673, Jeremy Barnes, Roman Klinger, Sabine Schulte, Lo\u00efc Barrault, Magdalena Biesialska, Ondrej Bojar, M. Costa-juss\u00e0, C. Federmann, Roman Graham, Barry Grundkiewicz, Haddow, Matthias Huck, E. Joanis, Philipp Tom Kocmi, Chi-kiu Koehn, Nikola kiu Lo, Christof Ljubesic, Makoto 689 Monz, Masaaki Morishita, Toshi-690 Nagata, Samuel R. Bowman, Gabor Angeli, Christopher Potts, Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-698, Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang, Saksham Singhal, Xian-Ling Mao, Heyan Huang, Furu Wei, Saksham Nan Yang, Jason Phang, Phu Iacer Calixto, M. Htut, Yada Pruk-830, Haokun Liu, Clara Vania, Katharina Kann, English, Telmo Pires, Eva Schlinger, Dan Garrette, Colin Raffel, Noam Shazeer, Adam Roberts, Sharan Lee, Michael Narang, Yanqi Matena, Zhou, Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Sebastian Ruder, Noah Constant, Jan Botha, Aditya Sid-854, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Graham Neubig, Tal Schuster, Ori Ram, R. Barzilay, Aditya Siddhant, Melvin Johnson, Naveen Henry Tsai, Jason Riesa, Ankur Bapna", "url": "https://www.semanticscholar.org/paper/c664e03db7c63d707a8f07cabd9c0febf558ceca", "relevance": 2, "abstract": "", "citations": 0}
{"title": "InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training", "year": 2020, "authors": "Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, M. Zhou", "url": "https://www.semanticscholar.org/paper/4ceff7472c04ee6d76bce89d61ba4b445d8dbf74", "relevance": 2, "abstract": "In this work, we present an information-theoretic framework that formulates cross-lingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. The unified view helps us to better understand the existing methods for learning cross-lingual representations. More importantly, inspired by the framework, we propose a new pre-training task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The code and pre-trained models are available at https://aka.ms/infoxlm.", "citations": 418}
{"title": "How Multilingual is Multilingual BERT?", "year": 2019, "authors": "Telmo Pires, Eva Schlinger, Dan Garrette", "url": "https://www.semanticscholar.org/paper/809cc93921e4698bde891475254ad6dfba33d03b", "relevance": 2, "abstract": "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.", "citations": 1601}
{"title": "Language Models are Multilingual Chain-of-Thought Reasoners", "year": 2022, "authors": "Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei", "url": "https://www.semanticscholar.org/paper/62f0db3a5ad5c795ec18fc7a6e7b01836809df57", "relevance": 2, "abstract": "We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.", "citations": 521}
{"title": "Incorporating Lexical and Syntactic Knowledge for Unsupervised Cross-Lingual Transfer", "year": 2024, "authors": "Jianyu Zheng, Fengfei Fan, Jianquan Li", "url": "https://www.semanticscholar.org/paper/3584bda21689bbfd70b1ce868655705ef4094623", "relevance": 2, "abstract": "Unsupervised cross-lingual transfer involves transferring knowledge between languages without explicit supervision. Although numerous studies have been conducted to improve performance in such tasks by focusing on cross-lingual knowledge, particularly lexical and syntactic knowledge, current approaches are limited as they only incorporate syntactic or lexical information. Since each type of information offers unique advantages and no previous attempts have combined both, we attempt to explore the potential of this approach. In this paper, we present a novel framework called \u201cLexicon-Syntax Enhanced Multilingual BERT\u201d that combines both lexical and syntactic knowledge. Specifically, we use Multilingual BERT (mBERT) as the base model and employ two techniques to enhance its learning capabilities. The code-switching technique is used to implicitly teach the model lexical alignment information, while a syntactic-based graph attention network is designed to help the model encode syntactic structure. To integrate both types of knowledge, we input code-switched sequences into both the syntactic module and the mBERT base model simultaneously. Our extensive experimental results demonstrate this framework can consistently outperform all baselines of zero-shot cross-lingual transfer, with the gains of 1.0 3.7 points on text classification, named entity recognition (ner), and semantic parsing tasks.", "citations": 3}
{"title": "Language Fusion for Parameter-Efficient Cross-lingual Transfer", "year": 2025, "authors": "Philipp Borchert, Ivan Vuli'c, Marie-Francine Moens, Jochen De Weerdt", "url": "https://www.semanticscholar.org/paper/880e601b962ae834ce973ca02cb19fbfad6bcdd9", "relevance": 2, "abstract": "Limited availability of multilingual text corpora for training language models often leads to poor performance on downstream tasks due to undertrained representation spaces for languages other than English. This 'under-representation' has motivated recent cross-lingual transfer methods to leverage the English representation space by e.g. mixing English and 'non-English' tokens at the input level or extending model parameters to accommodate new languages. However, these approaches often come at the cost of increased computational complexity. We propose Fusion forLanguage Representations (FLARE) in adapters, a novel method that enhances representation quality and downstream performance for languages other than English while maintaining parameter efficiency. FLARE integrates source and target language representations within low-rank (LoRA) adapters using lightweight linear transformations, maintaining parameter efficiency while improving transfer performance. A series of experiments across representative cross-lingual natural language understanding tasks, including natural language inference, question-answering and sentiment analysis, demonstrate FLARE's effectiveness. FLARE achieves performance improvements of 4.9% for Llama 3.1 and 2.2% for Gemma~2 compared to standard LoRA fine-tuning on question-answering tasks, as measured by the exact match metric.", "citations": 3}
{"title": "Unified Framework for Efficient Cross-Lingual Transfer Learning Across Low-Resource Languages Using Knowledge-Augmented Multilingual Models", "year": null, "authors": "Ritika Budhiraja, Bhaumik Tyagi, Sagar Kumar", "url": "https://www.semanticscholar.org/paper/674e5383f5f5f46abf976e7acfd314e0e929a2a6", "relevance": 2, "abstract": "", "citations": 0}
{"title": "Cross-Lingual Activation Steering for Multilingual Language Models", "year": 2026, "authors": "Rhitabrat Pokharel, Ameeta Agrawal, Tanay Nagar", "url": "https://www.semanticscholar.org/paper/484928598e96168100bcbc16bf9d3496ef034172", "relevance": 2, "abstract": "Large language models exhibit strong multilingual capabilities, yet significant performance gaps persist between dominant and non-dominant languages. Prior work attributes this gap to imbalances between shared and language-specific neurons in multilingual representations. We propose Cross-Lingual Activation Steering (CLAS), a training-free inference-time intervention that selectively modulates neuron activations. We evaluate CLAS on classification and generation benchmarks, achieving average improvements of 2.3% (Acc.) and 3.4% (F1) respectively, while maintaining high-resource language performance. We discover that effective transfer operates through functional divergence rather than strict alignment; performance gains correlate with increased language cluster separation. Our results demonstrate that targeted activation steering can unlock latent multilingual capacity in existing models without modification to model weights.", "citations": 1}
{"title": "MENLO: From Preferences to Proficiency - Evaluating and Modeling Native-like Quality Across 47 Languages", "year": 2025, "authors": "Chenxi Whitehouse, Sebastian Ruder, Tony Lin, Oksana Kurylo, Haruka Takagi, Janice Lam, Nicolo Busetto, Denise Diaz", "url": "https://www.semanticscholar.org/paper/51bab5985d33c72bae0f691443b9e58e9462f4d8", "relevance": 2, "abstract": "Ensuring native-like quality of large language model (LLM) responses across many languages is challenging. To address this, we introduce MENLO, a framework that operationalizes the evaluation of native-like response quality based on audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423 human-annotated prompt-response preference pairs covering four quality dimensions with high inter-annotator agreement in 47 language varieties. Our evaluation reveals that zero-shot LLM judges benefit significantly from pairwise evaluation and our structured annotation rubrics, yet they still underperform human annotators on our dataset. We demonstrate substantial improvements through fine-tuning with reinforcement learning, reward shaping, and multi-task learning approaches. Additionally, we show that RL-trained judges can serve as generative reward models to enhance LLMs'multilingual proficiency, though discrepancies with human judgment remain. Our findings suggest promising directions for scalable multilingual evaluation and preference alignment. We release our dataset and evaluation framework to support further research in multilingual LLM evaluation.", "citations": 1}
{"title": "Lifting the Curse of Multilinguality by Pre-training Modular Transformers", "year": 2022, "authors": "Jonas Pfeiffer, Naman Goyal, Xi Victoria Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe", "url": "https://www.semanticscholar.org/paper/c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6", "relevance": 2, "abstract": "Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns language-specific components post-hoc, we pre-train the modules of our Cross-lingual Modular (X-Mod) models from the start. Our experiments on natural language inference, named entity recognition and question answering show that our approach not only mitigates the negative interference between languages, but also enables positive transfer, resulting in improved monolingual and cross-lingual performance. Furthermore, our approach enables adding languages post-hoc with no measurable drop in performance, no longer limiting the model usage to the set of pre-trained languages.", "citations": 166}
{"title": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study", "year": 2019, "authors": "Karthikeyan K, Zihan Wang, Stephen Mayhew, Dan Roth", "url": "https://www.semanticscholar.org/paper/3b2538f84812f434c740115c185be3e5e216c526", "relevance": 2, "abstract": "Recent work has exhibited the surprising cross-lingual abilities of multilingual BERT (M-BERT) -- surprising since it is trained without any cross-lingual objective and with no aligned data. In this work, we provide a comprehensive study of the contribution of different components in M-BERT to its cross-lingual ability. We study the impact of linguistic properties of the languages, the architecture of the model, and the learning objectives. The experimental study is done in the context of three typologically different languages -- Spanish, Hindi, and Russian -- and using two conceptually different NLP tasks, textual entailment and named entity recognition. Among our key conclusions is the fact that the lexical overlap between languages plays a negligible role in the cross-lingual success, while the depth of the network is an integral part of it. All our models and implementations can be found on our project page: this http URL .", "citations": 368}
{"title": "Emerging Cross-lingual Structure in Pretrained Language Models", "year": 2019, "authors": "Shijie Wu, Alexis Conneau, Haoran Li, Luke Zettlemoyer, Veselin Stoyanov", "url": "https://www.semanticscholar.org/paper/cfe8ec7a183ed548db1a862e38908343cefb94c7", "relevance": 2, "abstract": "We study the problem of multilingual masked language modeling, i.e. the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer. We show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains. The only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder. To better understand this result, we also show that representations from monolingual BERT models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces. For multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process.", "citations": 300}
{"title": "Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges", "year": 2019, "authors": "N. Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, M. Krikun, M. Chen, Yuan Cao, George F. Foster, Colin Cherry, Wolfgang Macherey, Z. Chen, Yonghui Wu", "url": "https://www.semanticscholar.org/paper/c17985a669522e7e85ae3d34754c7df49c7187d1", "relevance": 2, "abstract": "We introduce our efforts towards building a universal neural machine translation (NMT) system capable of translating between any language pair. We set a milestone towards this goal by building a single massively multilingual NMT model handling 103 languages trained on over 25 billion examples. Our system demonstrates effective transfer learning ability, significantly improving translation quality of low-resource languages, while keeping high-resource language translation quality on-par with competitive bilingual baselines. We provide in-depth analysis of various aspects of model building that are crucial to achieving quality and practicality in universal NMT. While we prototype a high-quality universal translation system, our extensive empirical analysis exposes issues that need to be further addressed, and we suggest directions for future research.", "citations": 450}
{"title": "XNLI: Evaluating Cross-lingual Sentence Representations", "year": 2018, "authors": "Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman, Holger Schwenk, Veselin Stoyanov", "url": "https://www.semanticscholar.org/paper/1c3112ef8a346b9817382ed34a8c146c53d5bcf5", "relevance": 2, "abstract": "State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.", "citations": 1543}
{"title": "Unknown Script: Impact of Script on Cross-Lingual Transfer", "year": 2024, "authors": "Wondimagegnhue Tufa, Ilia Markov, Piek Vossen", "url": "https://www.semanticscholar.org/paper/05b6e5fabafb08283a7b89db45f72a40d5b4c7ad", "relevance": 2, "abstract": "Cross-lingual transfer has become an effective way of transferring knowledge between languages. In this paper, we explore an often overlooked aspect in this domain: the influence of the source language of a language model on language transfer performance. We consider a case where the target language and its script are not part of the pre-trained model. We conduct a series of experiments on monolingual and multilingual models that are pre-trained on different tokenization methods to determine factors that affect cross-lingual transfer to a new language with a unique script. Our findings reveal the importance of the tokenizer as a stronger factor than the shared script, language similarity, and model size.", "citations": 3}
{"title": "Understanding Cross-Lingual Alignment - A Survey", "year": 2024, "authors": "Katharina H\u00e4mmerl, Jindvrich Libovick'y, Alexander Fraser", "url": "https://www.semanticscholar.org/paper/c84fd9a445f0c9cbab900ec3638ad832fa4fde32", "relevance": 2, "abstract": "Cross-lingual alignment, the meaningful similarity of representations across languages in multilingual language models, has been an active field of research in recent years. We survey the literature of techniques to improve cross-lingual alignment, providing a taxonomy of methods and summarising insights from throughout the field. We present different understandings of cross-lingual alignment and their limitations. We provide a qualitative summary of results from a large number of surveyed papers. Finally, we discuss how these insights may be applied not only to encoder models, where this topic has been heavily studied, but also to encoder-decoder or even decoder-only models, and argue that an effective trade-off between language-neutral and language-specific information is key.", "citations": 31}
{"title": "PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment", "year": 2024, "authors": "Jiahuan Li, Shujian Huang, Xinyu Dai, Jiajun Chen", "url": "https://www.semanticscholar.org/paper/a3ca19771db42fa926f61d79eac4958b8a2e2b66", "relevance": 2, "abstract": "Large language models demonstrate reasonable multilingual abilities, despite predominantly English-centric pretraining. However, the spontaneous multilingual alignment in these models is shown to be weak, leading to unsatisfactory cross-lingual transfer and knowledge sharing. Previous works attempt to address this issue by explicitly injecting multilingual alignment information during or after pretraining. Thus for the early stage in pretraining, the alignment is weak for sharing information or knowledge across languages. In this paper, we propose PreAlign, a framework that establishes multilingual alignment prior to language model pretraining. PreAlign injects multilingual alignment by initializing the model to generate similar representations of aligned words and preserves this alignment using a code-switching strategy during pretraining. Extensive experiments in a synthetic English to English-Clone setting demonstrate that PreAlign significantly outperforms standard multilingual joint training in language modeling, zero-shot cross-lingual transfer, and cross-lingual knowledge application. Further experiments in real-world scenarios further validate PreAlign\u2019s effectiveness across various model sizes.", "citations": 17}
{"title": "Cross-lingual Transfer and Multilingual Learning for Detecting Harmful Behaviour in African Under-Resourced Language Dialogue", "year": 2024, "authors": "T. Ajayi, Mihael Arcan, P. Buitelaar", "url": "https://www.semanticscholar.org/paper/8d12b3065af0873503bc70c4c54b856cf493dd4a", "relevance": 2, "abstract": "Most harmful dialogue detection models are developed for high-resourced languages. Consequently, users who speak under-resourced languages cannot fully benefit from these models in terms of usage, development, detection and mitigation of harmful dialogue utterances. Our work aims at detecting harmful utterances in under-resourced African languages. We leverage transfer learning using pretrained models trained with multilingual embeddings to develop a cross-lingual model capable of detecting harmful content across various African languages. We first fine-tune a harmful dialogue detection model on a selected African dialogue dataset. Additionally, we fine-tune a model on a combined dataset in some African languages to develop a multilingual harmful dialogue detection model. We then evaluate the cross-lingual model\u2019s ability to generalise to an unseen African language by performing harmful dialogue detection in an under-resourced language not present during pretraining or fine-tuning. We evaluate our models on the test datasets. We show that our best performing models achieve impressive results in terms of F1 score. Finally, we discuss the results and limitations of our work.", "citations": 4}
{"title": "Improving Zero-Shot Cross-Lingual Transfer via Progressive Code-Switching", "year": 2024, "authors": "Zhuoran Li, Chunming Hu, J. Chen, Zhijun Chen, Xiaohui Guo, Richong Zhang", "url": "https://www.semanticscholar.org/paper/2a33dfcdd008599f3ed094ac73586cca9715a947", "relevance": 2, "abstract": "Code-switching is a data augmentation scheme mixing words from multiple languages into source lingual text. It has achieved considerable generalization performance of cross-lingual transfer tasks by aligning cross-lingual contextual word representations. However, uncontrolled and over-replaced code-switching would augment dirty samples to model training. In other words, the excessive code-switching text samples will negatively hurt the models' cross-lingual transferability. To this end, we propose a Progressive Code-Switching (PCS) method to gradually generate moderately difficult code-switching examples for the model to discriminate from easy to hard. The idea is to incorporate progressively the preceding learned multilingual knowledge using easier code-switching data to guide model optimization on succeeding harder code-switching data. Specifically, we first design a difficulty measurer to measure the impact of replacing each word in a sentence based on the word relevance score. Then a code-switcher generates the code-switching data of increasing difficulty via a controllable temperature variable. In addition, a training scheduler decides when to sample harder code-switching data for model training. Experiments show our model achieves state-of-the-art results on three different zero-shot cross-lingual transfer tasks across ten languages.", "citations": 8}
{"title": "Argument Mining in Data Scarce Settings: Cross-lingual Transfer and Few-shot Techniques", "year": 2024, "authors": "Anar Yeginbergen, Maite Oronoz, Rodrigo Agerri", "url": "https://www.semanticscholar.org/paper/b057fff0d4c23926ad041986bcb7288dd62bb900", "relevance": 2, "abstract": "Recent research on sequence labelling has been exploring different strategies to mitigate the lack of manually annotated data for the large majority of the world languages. Among others, the most successful approaches have been based on (i) the cross-lingual transfer capabilities of multilingual pre-trained language models (model-transfer), (ii) data translation and label projection (data-transfer) and (iii), prompt-based learning by reusing the mask objective to exploit the few-shot capabilities of pre-trained language models (few-shot). Previous work seems to conclude that model-transfer outperforms data-transfer methods and that few-shot techniques based on prompting are superior to updating the model's weights via fine-tuning. In this paper, we empirically demonstrate that, for Argument Mining, a sequence labelling task which requires the detection of long and complex discourse structures, previous insights on cross-lingual transfer or few-shot learning do not apply. Contrary to previous work, we show that for Argument Mining data transfer obtains better results than model-transfer and that fine-tuning outperforms few-shot methods. Regarding the former, the domain of the dataset used for data-transfer seems to be a deciding factor, while, for few-shot, the type of task (length and complexity of the sequence spans) and sampling method prove to be crucial.", "citations": 8}
{"title": "Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?", "year": 2024, "authors": "Shaoxiong Ji, Timothee Mickus, Vincent Segonne, J\u00f6rg Tiedemann", "url": "https://www.semanticscholar.org/paper/dbbd46753d98f2792076e038f1da2fd3e74b4eec", "relevance": 2, "abstract": "Multilingual pretraining and fine-tuning have remarkably succeeded in various natural language processing tasks. Transferring representations from one language to another is especially crucial for cross-lingual learning. One can expect machine translation objectives to be well suited to fostering such capabilities, as they involve the explicit alignment of semantically equivalent sentences from different languages. This paper investigates the potential benefits of employing machine translation as a continued training objective to enhance language representation learning, bridging multilingual pretraining and cross-lingual applications. We study this question through two lenses: a quantitative evaluation of the performance of existing models and an analysis of their latent representations. Our results show that, contrary to expectations, machine translation as the continued training fails to enhance cross-lingual representation learning in multiple cross-lingual natural language understanding tasks. We conclude that explicit sentence-level alignment in the cross-lingual scenario is detrimental to cross-lingual transfer pretraining, which has important implications for future cross-lingual transfer studies. We furthermore provide evidence through similarity measures and investigation of parameters that this lack of positive influence is due to output separability\u2014which we argue is of use for machine translation but detrimental elsewhere.", "citations": 6}
{"title": "Cross-lingual prompting method with semantic-based answer space clustering", "year": 2024, "authors": "Ahtamjan Ahmat, Yating Yang, Bo Ma, Rui Dong, Rong Ma, Lei Wang", "url": "https://www.semanticscholar.org/paper/20486cd9b9ad4426daa7fb6f3ddf1eb9f1accf32", "relevance": 2, "abstract": "", "citations": 0}
{"title": "Analyzing the Effect of Linguistic Similarity on Cross-Lingual Transfer: Tasks and Experimental Setups Matter", "year": 2025, "authors": "Verena Blaschke, Masha Fedzechkina, Maartje ter Hoeve", "url": "https://www.semanticscholar.org/paper/4c7d964da17a3d23cfe33d3def750915f31578a5", "relevance": 1, "abstract": "Cross-lingual transfer is a popular approach to increase the amount of training data for NLP tasks in a low-resource context. However, the best strategy to decide which cross-lingual data to include is unclear. Prior research often focuses on a small set of languages from a few language families and/or a single task. It is still an open question how these findings extend to a wider variety of languages and tasks. In this work, we analyze cross-lingual transfer for 263 languages from a wide variety of language families. Moreover, we include three popular NLP tasks: POS tagging, dependency parsing, and topic classification. Our findings indicate that the effect of linguistic similarity on transfer performance depends on a range of factors: the NLP task, the (mono- or multilingual) input representations, and the definition of linguistic similarity.", "citations": 8}
{"title": "Zero-Shot vs. Translation-Based Cross-Lingual Transfer: The Case of Lexical Gaps", "year": 2024, "authors": "Abteen Ebrahimi, K. Wense", "url": "https://www.semanticscholar.org/paper/bf79fc057b8c839ca3e56827b20dffd2eec9fa49", "relevance": 1, "abstract": "Cross-lingual transfer can be achieved through two main approaches: zero-shot transfer or machine translation (MT). While the former has been the dominant approach, both have been shown to be competitive. In this work, we compare the current performance and long-term viability of these methods. We leverage lexical gaps to create a multilingual question answering dataset, which provides a difficult domain for evaluation. Both approaches struggle in this setting, though zero-shot transfer performs better, as current MT outputs are not specific enough for the task. Using oracle translation offers the best performance, showing that this approach can perform well long-term, however current MT quality is a bottleneck. We also conduct an exploratory study to see if humans produce translations sufficient for the task with only general instructions. We find this to be true for the majority of translators, but not all. This indicates that while translation has the potential to outperform zero-shot approaches, creating MT models that generate accurate task-specific translations may not be straightforward.", "citations": 3}
{"title": "The Impact of Incidental Multilingual Text on Cross-Lingual Transfer in Monolingual Retrieval", "year": 2025, "authors": "Andrew Liu, Edward Xu, Crystina Zhang, Jimmy Lin", "url": "https://www.semanticscholar.org/paper/7ce8b176fef651b847be848480298ab368949c6a", "relevance": 1, "abstract": "", "citations": 1}
{"title": "UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages", "year": 2024, "authors": "Trinh Pham, Khoi M. Le, Anh Tuan Luu", "url": "https://www.semanticscholar.org/paper/b71884092949e1c3ff48944a0fe81b17a90dcc0d", "relevance": 1, "abstract": "In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with Optimized Embeddings and Vocabulary), a comprehensive approach developed to improve the effectiveness of Cross-Lingual Transfer Learning, particularly in languages with limited resources. Our approach tackles two essential elements of a language model: the initialization of embeddings and the optimal vocabulary size. Specifically, we propose a novel embedding initialization method that leverages both lexical and semantic alignment for a language. In addition, we present a method for systematically searching for the optimal vocabulary size, ensuring a balance between model complexity and linguistic coverage. Our experiments across multilingual datasets show that our approach greatly improves the F1-Score in several languages. UniBridge is a robust and adaptable solution for cross-lingual systems in various languages, highlighting the significance of initializing embeddings and choosing the right vocabulary size in cross-lingual environments.", "citations": 5}
{"title": "When Scripts Diverge: Strengthening Low-Resource Neural Machine Translation Through Phonetic Cross-Lingual Transfer", "year": 2025, "authors": "Ammon Shurtz, Christian Richardson, Stephen D. Richardson", "url": "https://www.semanticscholar.org/paper/529c3fab8aafb424cf64f744073d871fe336c1b5", "relevance": 1, "abstract": "Multilingual Neural Machine Translation (MNMT) models enhance translation quality for low-resource languages by exploiting cross-lingual similarities during training\u2014a process known as knowledge transfer. This transfer is particularly effective between languages that share lexical or structural features, often enabled by a common orthography. However, languages with strong phonetic and lexical similarities but distinct writing systems experience limited benefits, as the absence of a shared orthography hinders knowledge transfer. To address this limitation, we propose an approach based on phonetic information that enhances token-level alignment across scripts by leveraging transliterations. We systematically evaluate several phonetic transcription techniques and strategies for incorporating phonetic information into NMT models. Our results show that using a shared encoder to process orthographic and phonetic inputs separately consistently yields the best performance for Khmer, Thai, and Lao in both directions with English, and that our custom Cognate-Aware Transliteration (CAT) method consistently improves translation quality over the baseline.", "citations": 0}
{"title": "Contextual Label Projection for Cross-Lingual Structure Extraction", "year": 2023, "authors": "Tanmay Parekh, I-Hung Hsu, Kuan-Hao Huang, Kai-Wei Chang, Nanyun Peng", "url": "https://www.semanticscholar.org/paper/95450ce77a9458fa40f4a9169181d34af55139fc", "relevance": 1, "abstract": "", "citations": 4}
{"title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages", "year": 2020, "authors": "J. Clark, Eunsol Choi, Michael Collins, Dan Garrette, T. Kwiatkowski, Vitaly Nikolaev, J. Palomaki", "url": "https://www.semanticscholar.org/paper/83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6", "relevance": 1, "abstract": "Abstract Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA\u2014a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology\u2014the set of linguistic features each language expresses\u2014such that we expect models performing well on this set to generalize across a large number of the world\u2019s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don\u2019t know the answer yet, and the data is collected directly in each language without the use of translation.", "citations": 695}
{"title": "A Survey of Cross-Lingual Alignment: Definitions, Methods, Future", "year": null, "authors": "Multimodality, Jared D Subbiah, Prafulla Kaplan, A. Dhariwal, P. Neelakantan, Girish Shyam, Amanda Sastry, Sandhini Askell, Ariel Agarwal, Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, M. Sigler, Scott Litwin, Benjamin Gray, Chess, Alec Radford, I. Sutskever, Steven Cao, Nikita Kitaev, Dan Klein, Multi-757, T. Chang, Zhuowen Tu, Benjamin Bergen, Aditi Chaudhary, K. Raman, Krishna Srinivasan, Zewen Chi, Li Dong, Furu Wei, Saksham Nan Yang, Bo Zheng, Shaohan Huang, L. Mao, Heyan Huang, Im-781, Shuming Ma, Xian-Ling Mao, Won Chung, Thibault F\u00e9vry, Henry Tsai, Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. 2020, Alexis Conneau, Ruty Rinott, Guillaume Lample, A. Williams, Samuel Bowman, Holger Schwenk, Kevin Heffernan, Onur \u00c7elebi, John Hewitt, Edward J. Hu, Yelong Shen, Zeyuan Phillip Wallis, Junjie Hu, Melvin Johnson, Orhan Firat, Aditya Sid-949, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Kuan-Hao Huang, Wasi Ahmad, Nanyun Peng, Mihir Kale, Rami Al-Rfou, Linting, Noah Xue, Constant Melvin Johnson, Ivana Kvapil\u00edkov\u00e1, Mikel Artetxe, Gorka Labaka, Eneko Agirre, Ond\u02c7rej Bojar. 2020, Unsupervised, Marc\u2019Aurelio, Ludovic Denoyer, Herv\u00e9 J\u00e9gou, Davis Liang, Hila Gonen, Yun-ing Mao, Rui Hou, Xi Victoria, Todor Lin, Mikel Mihaylov, Artetxe, Tianlu, Shuohui Wang, Daniel Chen, Myle Simig, Na-865 Ott, Shruti Goyal, Jingfei Bhosale, Du, Ramakanth, Sam Pasunuru, Punit Singh Shleifer, Koura, Vishrav, Brian Chaudhary, Jeff O\u2019Horo, Luke Wang, Zettle-1023 Zornitsa, Mona Kozareva, Veselin Diab, Stoy-1024", "url": "https://www.semanticscholar.org/paper/7e01125313a35771ca4b379c502b817a7febb8c8", "relevance": 1, "abstract": "", "citations": 0}
{"title": "Cross-Lingual Transfer Learning for Low-Resource Speech Translation", "year": 2024, "authors": "Sameer Khurana, Nauman Dawalatabad, Antoine Laurent, Luis Vicente, Pablo Gimeno, Victoria Mingote, J. L. Roux, James Glass, Mitsubishi Electric, Research Lab", "url": "https://www.semanticscholar.org/paper/b986b9c90b6d2fac80468fb9d1101004ac1015aa", "relevance": 1, "abstract": "The paper presents a novel three-step transfer learning framework for enhancing cross-lingual transfer from high- to low-resource languages in the downstream application of Automatic Speech Translation. The approach integrates a semantic knowledge-distillation step into the existing two-step cross-lingual transfer learning framework XLS-R. This extra step aims to encode semantic knowledge in the multilingual speech encoder pre-trained via Self-Supervised Learning using unlabeled speech. Our proposed three-step cross-lingual transfer learning framework addresses the large cross-lingual transfer gap (TRFGap) observed in the XLS-R framework between high-resource and low-resource languages. We validate our proposal through extensive experiments and comparisons on the CoVoST-2 benchmark, showing significant improvements in translation performance, especially for low-resource languages, and a notable reduction in the TRFGap.", "citations": 5}
{"title": "Cross-Lingual Transfer from Related Languages: Treating Low-Resource Maltese as Multilingual Code-Switching", "year": 2024, "authors": "Kurt Micallef, Nizar Habash, Claudia Borg, Fadhl Eryani, Houda Bouamor", "url": "https://www.semanticscholar.org/paper/91b57fdc7d80ea8f8869bc2979a4240fff2f8136", "relevance": 1, "abstract": "Although multilingual language models exhibit impressive cross-lingual transfer capabilities on unseen languages, the performance on downstream tasks is impacted when there is a script disparity with the languages used in the multilingual model\u2019s pre-training data. Using transliteration offers a straightforward yet effective means to align the script of a resource-rich language with a target language thereby enhancing cross-lingual transfer capabilities. However, for mixed languages, this approach is suboptimal, since only a subset of the language benefits from the cross-lingual transfer while the remainder is impeded. In this work, we focus on Maltese, a Semitic language, with substantial influences from Arabic, Italian, and English, and notably written in Latin script. We present a novel dataset annotated with word-level etymology. We use this dataset to train a classifier that enables us to make informed decisions regarding the appropriate processing of each token in the Maltese language. We contrast indiscriminate transliteration or translation to mixing processing pipelines that only transliterate words of Arabic origin, thereby resulting in text with a mixture of scripts. We fine-tune the processed data on four downstream tasks and show that conditional transliteration based on word etymology yields the best results, surpassing fine-tuning with raw Maltese or Maltese processed with non-selective pipelines.", "citations": 7}
{"title": "Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages", "year": 2022, "authors": "Kevin Heffernan, Onur cCelebi, Holger Schwenk", "url": "https://www.semanticscholar.org/paper/57aba6d26fec16e98895a7ecabac862603ca45ec", "relevance": 1, "abstract": "Scaling multilingual representation learning beyond the hundred most frequent languages is challenging, in particular to cover the long tail of low-resource languages. A promising approach has been to train one-for-all multilingual models capable of cross-lingual transfer, but these models often suffer from insufficient capacity and interference between unrelated languages. Instead, we move away from this approach and focus on training multiple language (family) specific representations, but most prominently enable all languages to still be encoded in the same representational space. To achieve this, we focus on teacher-student training, allowing all encoders to be mutually compatible for bitext mining, and enabling fast learning of new languages. We introduce a new teacher-student training scheme which combines supervised and self-supervised training, allowing encoders to take advantage of monolingual training data, which is valuable in the low-resource setting. Our approach significantly outperforms the original LASER encoder. We study very low-resource languages and handle 50 African languages, many of which are not covered by any other model. For these languages, we train sentence encoders, mine bitexts, and validate the bitexts by training NMT systems.", "citations": 66}
{"title": "Cross-Lingual Transfer Learning for Speech Translation", "year": 2024, "authors": "Rao Ma, Yassir Fathullah, Mengjie Qian, Siyuan Tang, Mark J. F. Gales, Kate Knill", "url": "https://www.semanticscholar.org/paper/b7b836e33c68166bde2b0ca53a10a2c3625a2bb4", "relevance": 1, "abstract": "There has been increasing interest in building multilingual foundation models for NLP and speech research. This paper examines how to expand the speech translation capability of these models with restricted data. Whisper, a speech foundation model with strong performance on speech recognition and English translation, is used as the example model. Using speech-to-speech retrieval to analyse the audio representations generated by the encoder, we show that utterances from different languages are mapped to a shared semantic space. This shared embedding space can then be leveraged for zero-shot cross-lingual transfer in speech translation. By fine-tuning the Whisper decoder with only English-to-Chinese speech translation data, improved performance for translation to Chinese can be obtained for multiple languages, in addition to English. Furthermore, for languages related to those seen in training it is possible to perform speech translation, despite the model never seeing the language in training, or being able to perform transcription.", "citations": 17}
{"title": "Cross-Lingual Transfer Learning in RNNs for Enhancing Linguistic Diversity in Natural Language Processing", "year": 2024, "authors": "Shikha Tiwari, Ch. Meher Babu, P. Shanker, S. V, Vandana Roy, Ramgopal Kashyap", "url": "https://www.semanticscholar.org/paper/01cc305e1a9d7c7246032ee627a71dee712483af", "relevance": 1, "abstract": "In addition to removing biases, these findings reveal a comprehensive method for improving cross-lingual transfer learning in NLP, which in turn makes language more diverse. This approach was created as a result of this effort. In order to construct the system utilizing these methodologies, five fundamental procedures are followed. One approach that falls under this category is MTI, which stands for Multilingual Embedding Adaptation and is used for Machine Translation-based Initialization. Ethical Bias Mitigation, Linguistic Feature Alignment, and Recurrent Neural Network Adaptation are some supplementary methods. Developing each strategy to address the wide variety of issues that may arise during cross-lingual transfer learning required considerable deliberation. Starting with translated data, the MTI algorithm aligns the source and target language embeddings, the RNA algorithm aligns RNN structures, the LFA algorithm aligns linguistic characteristics, and the EBM method lowers biases fairly. The desired outcomes are attained by employing all these techniques. The outcomes of ablation research demonstrated the significance of each approach and their interdependence in ensuring the system's functionality. The framework provides a comprehensive strategy for increasing the linguistic diversity of natural language processing (NLP) models to accommodate individuals from varying cultural backgrounds and language abilities. Issues of social justice and equity receive substantial consideration within the framework.", "citations": 20}
