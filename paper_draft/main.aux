\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wendler2024llamas}
\citation{zhang2024multilingual}
\citation{etxaniz2023multilingual}
\citation{shi2022language}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{wendler2024llamas}
\citation{zhang2024multilingual}
\citation{etxaniz2023multilingual}
\citation{huang2023languages}
\citation{shi2022language}
\citation{ahuja2023mega}
\citation{bandarkar2023belebele}
\citation{hu2020xtreme}
\citation{wei2022chain}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related_work}{{2}{2}{Related Work}{section.2}{}}
\citation{shi2022language}
\citation{bandarkar2023belebele}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Language selection across resource tiers. Bengali is used for \textsc  {MGSM}\xspace  and Hindi for \textsc  {Belebele}\xspace  in the low-resource tier based on benchmark availability.}}{3}{table.1}\protected@file@percent }
\newlabel{tab:languages}{{1}{3}{Language selection across resource tiers. Bengali is used for \mgsm and Hindi for \belebele in the low-resource tier based on benchmark availability}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\newlabel{sec:methodology}{{3}{3}{Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Experimental Design}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:design}{{3.1}{3}{Experimental Design}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Models}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:models}{{3.2}{3}{Models}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Benchmarks}{3}{subsection.3.3}\protected@file@percent }
\newlabel{sec:benchmarks}{{3.3}{3}{Benchmarks}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Languages}{3}{subsection.3.4}\protected@file@percent }
\newlabel{sec:languages}{{3.4}{3}{Languages}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Prompting Strategies}{3}{subsection.3.5}\protected@file@percent }
\newlabel{sec:strategies}{{3.5}{3}{Prompting Strategies}{subsection.3.5}{}}
\citation{huang2023languages}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textsc  {MGSM}\xspace  accuracy by model, strategy, and language. \textsc  {Claude Sonnet\nobreakspace  {}4}\xspace  achieves strong performance under \textsc  {Direct}\xspace  inference with no chain-of-thought, while \textsc  {GPT-4.1}\xspace  requires explicit reasoning steps. Bold indicates best per-column result.}}{4}{table.2}\protected@file@percent }
\newlabel{tab:mgsm_results}{{2}{4}{\mgsm accuracy by model, strategy, and language. \claudemodel achieves strong performance under \directstrat inference with no chain-of-thought, while \gptmodel requires explicit reasoning steps. Bold indicates best per-column result}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Evaluation Metrics}{4}{subsection.3.6}\protected@file@percent }
\newlabel{sec:metrics}{{3.6}{4}{Evaluation Metrics}{subsection.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Statistical Analysis}{4}{subsection.3.7}\protected@file@percent }
\newlabel{sec:stats}{{3.7}{4}{Statistical Analysis}{subsection.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{4}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{4}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Overall Accuracy}{4}{subsection.4.1}\protected@file@percent }
\newlabel{sec:overall}{{4.1}{4}{Overall Accuracy}{subsection.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textsc  {Belebele}\xspace  accuracy by model, strategy, and language. Both models achieve near-ceiling performance, with Hindi as the consistent weak spot. Bold indicates best per-column result.}}{5}{table.3}\protected@file@percent }
\newlabel{tab:belebele_results}{{3}{5}{\belebele accuracy by model, strategy, and language. Both models achieve near-ceiling performance, with Hindi as the consistent weak spot. Bold indicates best per-column result}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces H1: Performance gap between English and non-English languages. Negative gaps indicate non-English languages outperform English. \textsc  {GPT-4.1}\xspace  \textsc  {Self-Translate}\xspace  shows a large negative gap because \textsc  {Direct}\xspace  English accuracy (0.64) is depressed by the no-reasoning prompt format. Significance: * $p < 0.05$, ** $p < 0.01$, *** $p < 0.001$.}}{5}{table.4}\protected@file@percent }
\newlabel{tab:h1_results}{{4}{5}{H1: Performance gap between English and non-English languages. Negative gaps indicate non-English languages outperform English. \gptmodel \selftranslate shows a large negative gap because \directstrat English accuracy (0.64) is depressed by the no-reasoning prompt format. Significance: * $p < 0.05$, ** $p < 0.01$, *** $p < 0.001$}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}H1: English Advantage}{5}{subsection.4.2}\protected@file@percent }
\newlabel{sec:h1}{{4.2}{5}{H1: English Advantage}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}H2: English-Pivoting Strategies}{5}{subsection.4.3}\protected@file@percent }
\newlabel{sec:h2}{{4.3}{5}{H2: English-Pivoting Strategies}{subsection.4.3}{}}
\citation{ahuja2023mega}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces H2: Strategy lift over \textsc  {Direct}\xspace  for non-English languages. \textsc  {GPT-4.1}\xspace  shows large lift on \textsc  {MGSM}\xspace  , but this is attributable to chain-of-thought reasoning (English itself improves from 0.64 to 0.94). \textsc  {Claude Sonnet\nobreakspace  {}4}\xspace  shows no benefit from English pivoting.}}{6}{table.5}\protected@file@percent }
\newlabel{tab:h2_results}{{5}{6}{H2: Strategy lift over \directstrat for non-English languages. \gptmodel shows large lift on \mgsm , but this is attributable to chain-of-thought reasoning (English itself improves from 0.64 to 0.94). \claudemodel shows no benefit from English pivoting}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces H3: Spearman correlation between language resource level and \textsc  {Direct}\xspace  accuracy. No significant correlations are observed, suggesting resource level does not predict performance for frontier models.}}{6}{table.6}\protected@file@percent }
\newlabel{tab:h3_results}{{6}{6}{H3: Spearman correlation between language resource level and \directstrat accuracy. No significant correlations are observed, suggesting resource level does not predict performance for frontier models}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}H3: Resource Level Correlation}{6}{subsection.4.4}\protected@file@percent }
\newlabel{sec:h3}{{4.4}{6}{H3: Resource Level Correlation}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}H4: Model Comparison}{6}{subsection.4.5}\protected@file@percent }
\newlabel{sec:h4}{{4.5}{6}{H4: Model Comparison}{subsection.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Democratization Scores}{6}{subsection.4.6}\protected@file@percent }
\newlabel{sec:democratization}{{4.6}{6}{Democratization Scores}{subsection.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{6}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{6}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}The Disappearing English Advantage}{6}{subsection.5.1}\protected@file@percent }
\citation{wendler2024llamas,zhang2024multilingual}
\citation{etxaniz2023multilingual}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Model comparison on \textsc  {MGSM}\xspace  (left) and \textsc  {Belebele}\xspace  (right) under all three prompting strategies. \textsc  {Claude Sonnet\nobreakspace  {}4}\xspace  shows more consistent cross-language performance, while \textsc  {GPT-4.1}\xspace  benefits substantially from explicit reasoning scaffolding on \textsc  {MGSM}\xspace  .}}{7}{figure.1}\protected@file@percent }
\newlabel{fig:model_comparison}{{1}{7}{Model comparison on \mgsm (left) and \belebele (right) under all three prompting strategies. \claudemodel shows more consistent cross-language performance, while \gptmodel benefits substantially from explicit reasoning scaffolding on \mgsm }{figure.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Democratization scores (average accuracy / best-language accuracy). Higher is better; 1.0 indicates perfect equity. \textsc  {Claude Sonnet\nobreakspace  {}4}\xspace  achieves consistently higher equity across conditions. Bold indicates best per-row result.}}{7}{table.7}\protected@file@percent }
\newlabel{tab:democratization}{{7}{7}{Democratization scores (average accuracy / best-language accuracy). Higher is better; 1.0 indicates perfect equity. \claudemodel achieves consistently higher equity across conditions. Bold indicates best per-row result}{table.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Chain-of-Thought, Not Translation}{7}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Hindi as Persistent Challenge}{7}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Implications for Deployment}{7}{subsection.5.4}\protected@file@percent }
\citation{wendler2024llamas,etxaniz2023multilingual}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{ahuja2023mega}{{1}{2023}{{Ahuja et~al.}}{{Ahuja, Diddee, Hada, Jhamtani, Kakwani, Kulkarni, Seshadri, et~al.}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Limitations}{8}{subsection.5.5}\protected@file@percent }
\newlabel{sec:limitations}{{5.5}{8}{Limitations}{subsection.5.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{8}{Conclusion}{section.6}{}}
\bibcite{bandarkar2023belebele}{{2}{2023}{{Bandarkar et~al.}}{{Bandarkar, Liang, Muller, Artetxe, Shukla, Husa, Gober, Sridhar, et~al.}}}
\bibcite{etxaniz2023multilingual}{{3}{2023}{{Etxaniz et~al.}}{{Etxaniz, Azkune, Soroa, de~Lacalle, and Artetxe}}}
\bibcite{hu2020xtreme}{{4}{2020}{{Hu et~al.}}{{Hu, Ruder, Siddhant, Neubig, Firat, and Johnson}}}
\bibcite{huang2023languages}{{5}{2023}{{Huang et~al.}}{{Huang, Tang, Zhang, Zhao, Song, Xia, and Wei}}}
\bibcite{shi2022language}{{6}{2022}{{Shi et~al.}}{{Shi, Suzgun, Freitag, Wang, Srivats, Vosoughi, Chung, Tay, et~al.}}}
\bibcite{wei2022chain}{{7}{2022}{{Wei et~al.}}{{Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou}}}
\bibcite{wendler2024llamas}{{8}{2024}{{Wendler et~al.}}{{Wendler, Veselovsky, Monea, and West}}}
\bibcite{zhang2024multilingual}{{9}{2024}{{Zhang et~al.}}{{Zhang, Zhao, Li, Hao, Qin, and Liu}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Prompt Templates}{10}{appendix.A}\protected@file@percent }
\newlabel{appendix:prompts}{{A}{10}{Prompt Templates}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}\textsc  {MGSM}\xspace  Prompt Templates}{10}{subsection.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}\textsc  {Belebele}\xspace  Prompt Templates}{10}{subsection.A.2}\protected@file@percent }
\gdef \@abspage@last{11}
