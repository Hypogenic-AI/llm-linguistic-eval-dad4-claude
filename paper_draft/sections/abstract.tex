\begin{abstract}
Large language models are deployed globally, yet evaluations of their multilingual capabilities remain predominantly English-centric. We present a systematic evaluation of two frontier LLMs---\gptmodel and \claudemodel---across 8 languages, 3 prompting strategies, and 2 standardized benchmarks (\mgsm math reasoning and \belebele reading comprehension), comprising 4{,}800 real API calls. Our central finding is that \textbf{the English performance advantage has largely disappeared in frontier models}: \claudemodel achieves near-perfect cross-language equity (democratization score of 0.97) and actually scores higher on German (0.98) and Chinese (0.96) than English (0.92) for math reasoning. We further find that explicit English-pivoting strategies---self-translation and English chain-of-thought---provide no meaningful benefit for either model once we control for the chain-of-thought reasoning effect, contradicting earlier findings on GPT-3.5/4 and Llama-2. Language resource level does not predict performance in our data ($\rho = -0.09$ to $0.69$, all $p > 0.08$). These results suggest that the implicit English-processing bottleneck documented in earlier work has been substantially mitigated in 2025--2026 frontier models, with important implications for equitable global deployment.
\end{abstract}
